{
"cells": [
 {
		   "cell_type": "markdown",
	   "metadata": {},
	   "source": [
       "# Chapter 15: INTRODUCTION TO INFORMATION THEORY"
	   ]
	},
{
		   "cell_type": "markdown",
		   "metadata": {},
		   "source": [
			"## Example 15.1: problem1.sce"
		   ]
		  },
  {
"cell_type": "code",
	   "execution_count": null,
	   "metadata": {
	    "collapsed": true
	   },
	   "outputs": [],
"source": [
"//page no 687\n",
"// prob no 15.1\n",
"// Here we have given six messages. For 4-ary Huffman code, we need to add one dummy variable to satisfy the required condition of r+k(r-1) messages.\n",
"//probabilities are given as p(1)=0.3; p(2)=0.25; p(3)=0.15; p(4)=0.12; p(5)=0.1; p(6)=0.08; p(7)=0.\n",
"\n",
"//The length L of this code is calculated as\n",
"clc;\n",
"  \n",
"  n=input('enter the length of probability vector p, n= ');\n",
"p=[.3 .25 .15 .12 .1 .08 0];// enter probabilities in descending order\n",
"l=[1 1 1 2 2 2 2];// code length of individual message according to order\n",
"L=0;\n",
"for i=1:n\n",
"  L=L+(p(i)*l(i));\n",
"end\n",
"disp(+'4-ary digits',L,'Length = ');\n",
"\n",
"// Entropy of source is calculated as\n",
"H=0;\n",
"for i=1:n-1//since the value of log(1/0) for the last entry is infinite which when multiply by 0 gives result as 0\n",
"  H=H+(p(i)*log(1/p(i)));\n",
"end\n",
"H1=H/log(4)\n",
"disp(+'4-ary units',H1,'Entropy of source is, H = ');\n",
"\n",
"// Efficiency of code is given as \n",
"N=H1/L;\n",
"disp(N,'Efficiency of code, N = ');"
   ]
   }
,
{
		   "cell_type": "markdown",
		   "metadata": {},
		   "source": [
			"## Example 15.2: problem2.sce"
		   ]
		  },
  {
"cell_type": "code",
	   "execution_count": null,
	   "metadata": {
	    "collapsed": true
	   },
	   "outputs": [],
"source": [
"// Page no 688\n",
"// Example no. 15.2\n",
"// N=1\n",
"//Here we have given two messages with probabilities m1=0.8 and m2=0.2 . Therefore, Huffman code for the source is simply 0 and 1.\n",
"\n",
"//The length L of this code is calculated as\n",
"clear;\n",
"clc;\n",
"close; \n",
"N=1;\n",
"p=[.8 .2];//enter probabilities in descending order\n",
"n=length(p)\n",
"l=[1 1];//code length of individual message according to order\n",
"L=0;\n",
"for i=1:n\n",
"  L=L+(p(i)*l(i));\n",
"end\n",
"disp(L,'Length = ');\n",
"\n",
"// Entropy of source is calculated as\n",
"H=0;\n",
"for i=1:n\n",
"  H=H+(p(i)*log2(1/p(i)));\n",
"end\n",
"disp(+'bit',H,'Entropy of source is, H = ');\n",
"\n",
"// Efficiency of code is given as \n",
"N1=H/L;\n",
"disp(N1,'Efficiency of code, N = ');\n",
"\n",
"//for N=2\n",
"//There are four (2^N) combinations and their probabilities obtained by multiplying individuals probability.\n",
"//The length L of this code is calculated as\n",
"N=2;\n",
"p=[0.64 0.16 0.16 0.04];//enter probabilities in descending order\n",
"n=length(p);\n",
"l=[1 2 3 3];//code length of individual message according to order\n",
"L1=0;\n",
"for i=1:n\n",
"  L1=L1+(p(i)*l(i));\n",
"end\n",
"L=L1/N;// word length per message\n",
"disp(L,'Length = ');\n",
"\n",
"// Efficiency of code is given as \n",
"N2=H/L;\n",
"disp(N2,'Efficiency of code, N = ');\n",
"\n",
"\n",
"//for N=3\n",
"//There are eight (2^N)combinations and their probabilities obtained by multiplying individuals probability\n",
"//The length L of this code is calculated as\n",
"N=3;\n",
"p=[.512 .128 .128 .128 .032 .032 .032 .008];//enter probabilities in descending order\n",
"n=length(p);\n",
"l=[1 3 3 3 5 5 5 5];//code length of individual message according to order\n",
"L1=0;\n",
"for i=1:n\n",
"  L1=L1+(p(i)*l(i));\n",
"end\n",
"L=L1/N;// word length per message\n",
"disp(L,'Length = ');\n",
"\n",
"// Efficiency of code is given as \n",
"N3=H/L;\n",
"disp(N3,'Efficiency of code, N = ');"
   ]
   }
,
{
		   "cell_type": "markdown",
		   "metadata": {},
		   "source": [
			"## Example 15.4: problem4.sce"
		   ]
		  },
  {
"cell_type": "code",
	   "execution_count": null,
	   "metadata": {
	    "collapsed": true
	   },
	   "outputs": [],
"source": [
"// page no 702\n",
"// prob no 15.4\n",
"clc;\n",
"x0=(-1);x1=1;//given\n",
"y0=(-2);y1=2;//given\n",
"G=2;//gain of amplifier\n",
"//the probbilities are given as P(x)=1/2 for |x|<1 & P(y)=1/4 for |y<2| otherwise P(x)=P(y)=0.\n",
"//P(x<1 & -x<1)=1/2;\n",
"//P(y<2 & -y<2)=1/4;\n",
"// hence entropies are given as\n",
"g1=(1/2)*log2(2);\n",
"g2=(1/4)*log2(4); \n",
"X=integrate('g1*1','x',x0,x1);\n",
"Y=integrate('g2*1','y',y0,y1);\n",
"disp(+'bit',X,'entropy = ');\n",
"disp(+'bits',Y,'entropy = ');\n",
"//Here the entropy of random variable 'y' is twice that of the 'x'.This results may come as a surprise,since a knowledge of 'x' uniquely determines 'y' and vice versa , since y=2x.Hence , the average uncertainty of x and y should be identical.\n",
"// The reference entropy R1 for x is -log dx ,and The reference entropy R2 for y is -log dy (in the limit as dx,dy->0 ).\n",
"// R1= lim (dx->0) -log dx\n",
"//R2= lim (dy->0) -log dy\n",
"//and R1-R2 = lim(dx,dy->0) log(dx/dy) = log (dy/dx) = log2 2 =1 bit\n",
"//Therefore,the reference entropy of x is higher than the reference entropy for y. Hence we conclude that \n",
"disp(' if x and y have equal absolute entropies,their relative (differential) entropies must differ by 1 bit ');"
   ]
   }
],
"metadata": {
		  "kernelspec": {
		   "display_name": "Scilab",
		   "language": "scilab",
		   "name": "scilab"
		  },
		  "language_info": {
		   "file_extension": ".sce",
		   "help_links": [
			{
			 "text": "MetaKernel Magics",
			 "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
			}
		   ],
		   "mimetype": "text/x-octave",
		   "name": "scilab",
		   "version": "0.7.1"
		  }
		 },
		 "nbformat": 4,
		 "nbformat_minor": 0
}
